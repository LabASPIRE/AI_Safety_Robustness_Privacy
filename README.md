# üó°ASPIRE Labüõ°Ô∏è

## <a name="introduction"></a> ‚ò∫Ô∏èIntroduction

**Welcome to the official GitHub repository of ASPIRE Lab!**

**üî≠ About ASPIRE**

We are the ASPIRE Lab of ShanghaiTech University. ASPIRE is derived from "**A**I **S**ecurity, **P**r**I**vacy, and **R**obustn**E**ss." 

Our primary research focus is on the privacy and security of artificial intelligence, particularly in the fields of natural language processing and computer vision. We delve into topics such as defense and attack strategies concerning large language models and multimodal architectures.

**üö© About this repo**
- This is the official GitHub repository of ASPIRE Lab, containing the datasetsüìä, papersüìë, tool modelsü§ñ, and the weekly ReadingGroup materialsüìñ.
- You can also find the resources of our works here in the future!üåü

**Let‚Äôs start now!**

---

## <a name="table-of-contents">üß≠Table of Contents

- [‚ò∫Ô∏èIntroduction](#introduction)
- [üöÄTable of Contents](#table-of-contents)
- [üìäDatasets](#datasets)
- [üìñReading Group Materials](#reading-group-materials)
- [ü§ñTool Models](#tool-models)
 


---
## üìäDatasets


Work in progress...

---
## üìñReading Group Materials
**In this section, we list the materials of our weekly reading group. You can find the papers and presentation slides here!**
| Date  |Institute| Publication |        Paper        |    Slides   |
|:-----:|:--------:|:-----------:|:------------------|:-----------:|
| 23.10 | TJU&THU | EMNLP 2023  | [DEPN: Detecting and Editing Privacy Neurons in Pretrained Language Models](https://arxiv.org/pdf/2310.20138.pdf)|[24.03.01](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/DEPN.pdf)|
| 24.02 |UIUC| ArXiv | [HarmBench: A Standardized Evaluation Framework for Automated Red Teaming and Robust Refusal](https://arxiv.org/pdf/2402.04249.pdf)|[24.03.08](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/HarmBench.pdf)|
| 24.02 | UCSB | ArXiv |[Defending Large Language Models Against Jailbreak Attacks via Semantic Smoothing](https://arxiv.org/pdf/2402.16192.pdf)|[24.03.08](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/Semantic%20Smoothing%20.pdf)|
| 24.02 |University of Washington| ArXiv |[SafeDecoding: Defending against Jailbreak Attacks via Safety-Aware Decoding](https://arxiv.org/pdf/2402.08983.pdf)|[24.03.15](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/SafeDecoding%26PANDORA.pdf)|
| 24.02 |NTU| ArXiv |[PANDORA: Jailbreak GPTs by Retrieval Augmented Generation Poisoning](https://arxiv.org/pdf/2402.08416.pdf)|[24.03.15](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/SafeDecoding%26PANDORA.pdf)|
| 24.02 |Google DeepMind| ArXiv |[Stealing Part of a Production Language Model](https://arxiv.org/pdf/2403.06634.pdf)|[24.03.22](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/Stealing%20Part%20of%20a%20Production%20Language%20Model.pdf)|
| 24.03 |PKU| ArXiv |[AdaShield: Safeguarding Multimodal Large Language Models from Structure-based Attack via Adaptive Shield Prompting](https://arxiv.org/pdf/2403.09513.pdf)|[24.03.22](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/Adashield.pdf)|
| 24.03 |UIUC| ArXiv |[RigorLLM: Resilient Guardrails for Large Language Models against Undesired Content](https://arxiv.org/pdf/2403.13031.pdf)|[24.03.29](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/RigorLLM%26HADES.pdf)|
| 24.03 |Renmin University| ArXiv |[Images are Achilles' Heel of Alignment: Exploiting Visual Vulnerabilities for Jailbreaking Multimodal Large Language Models](https://arxiv.org/pdf/2403.09792.pdf)|[24.03.29](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/RigorLLM%26HADES.pdf)|
| 23.10 |UMD| CVPR2024 |[HALLUSIONBENCH: An Advanced Diagnostic Suite for Entangled Language Hallucination and Visual Illusion in Large Vision-Language Models](https://arxiv.org/pdf/2310.14566.pdf)|[24.03.29](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/HallusionBench.pdf)|
| 23.10 |SUTD&SeaAI| NIPS2023 |[AttackVLM: OnEvaluating Adversarial Robustness of Large Vision-Language Models](https://arxiv.org/pdf/2310.14566.pdf)|[24.04.12](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/AttackVLM.pdf)|
| 24.04 |The Ohio State University| ArXiv |[AmpleGCG: Learning a Universal and Transferable Generative Model of Adversarial Suffixes for Jailbreaking Both Open and Closed LLMs](https://arxiv.org/pdf/2404.07921.pdf)|[24.04.19](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/RethinkEval%26AmpleGCG.pdf)|
| 24.04 |Purdue University| ArXiv |[Rethinking How to Evaluate Language Model Jailbreak](https://arxiv.org/pdf/2404.06407.pdf)|[24.04.19](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/RethinkEval%26AmpleGCG.pdf)|
| 24.04 |South China University of Technology| ArXiv |[Eraser: Jailbreaking Defense in Large Language Models via Unlearning Harmful Knowledge](https://arxiv.org/pdf/2404.05880.pdf)|[24.04.19](https://github.com/LabASPIRE/LabASPIRE/blob/main/ReadingGroup/Slide/Eraser.pdf)|


---
## ü§ñTool Models
Work in progress...

---
**[‚¨Ü Back to ToC](#table-of-contents)**
